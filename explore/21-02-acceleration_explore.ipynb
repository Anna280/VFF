{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ball = pd.read_csv(\"/Users/annadaugaard/Desktop/VFF/explore/labelled_match_ball_match.csv\", index_col=0)\n",
    "event_data = pd.read_csv(\"/Users/annadaugaard/Desktop/VFF/raw_data/sample_match_1/Sample_Game_1_RawEventsData.csv\")\n",
    "event_data_passes = event_data[event_data[\"Type\"] == \"PASS\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(event_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data_passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data_passes[\"From\"] = (event_data_passes[\"From\"].astype(str).str.replace(\"Player\", \"\", regex=True)).astype(int)\n",
    "event_data_passes[\"To\"] = (event_data_passes[\"To\"].astype(str).str.replace(\"Player\", \"\", regex=True)).astype(int)\n",
    "event_data_passes[\"Start X\"] = (event_data_passes[\"Start X\"]).astype(float) * 106\n",
    "event_data_passes[\"End X\"] = (event_data_passes[\"End X\"]).astype(float) * 106\n",
    "event_data_passes[\"Start Y\"] = (event_data_passes[\"Start Y\"]).astype(float) * 68\n",
    "event_data_passes[\"End Y\"] = (event_data_passes[\"End Y\"]).astype(float) * 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ball = ball.dropna()\n",
    "# Calculate differences to compute speed\n",
    "ball['dx'] = ball['ball_x'].diff()\n",
    "ball['dy'] = ball['ball_y'].diff()\n",
    "ball['dt'] = ball['time'].diff()\n",
    "\n",
    "# Calculate speed (Euclidean distance per time difference)\n",
    "ball['speed'] = np.sqrt(ball['dx']**2 + ball['dy']**2) / ball['dt']\n",
    "\n",
    "ball['acceleration'] = ball[\"speed\"].diff() / ball['time'].diff()\n",
    "ball = ball[ball['speed'] <= 36]\n",
    "# Drop intermediate calculation columns from the cleaned tracking data\n",
    "ball.drop(columns=['dx', 'dy', 'dt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_pass_events_by_timestamp_coverage(pass_df, time_df, step=0.04, missing_threshold=0.30):\n",
    "    \"\"\"\n",
    "    Filters pass events based on timestamp coverage.\n",
    "    \n",
    "    For each pass event in pass_df (with columns \"Start Time [s]\" and \"End Time [s]\"),\n",
    "    a sequence of expected timestamps is generated from start to end with intervals of 'step' seconds.\n",
    "    The function then checks how many of these expected timestamps are present in time_df[\"time\"].\n",
    "    If more than missing_threshold (default 30%) of the expected timestamps are missing, the event is discarded.\n",
    "    \n",
    "    Parameters:\n",
    "      pass_df (pd.DataFrame): DataFrame containing pass events with \"Start Time [s]\" and \"End Time [s]\".\n",
    "      time_df (pd.DataFrame): DataFrame containing available timestamps in a column named \"time\".\n",
    "      step (float): The time step for generating expected timestamps (default 0.04).\n",
    "      missing_threshold (float): Maximum allowable fraction of missing timestamps (default 0.30).\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: A filtered version of pass_df.\n",
    "    \"\"\"\n",
    "    # Create a set of available timestamps from time_df, rounding to 2 decimals\n",
    "    available_times = set(np.round(time_df[\"time\"].values, 2))\n",
    "    \n",
    "    filtered_rows = []\n",
    "    for _, row in pass_df.iterrows():\n",
    "        start = row[\"Start Time [s]\"]\n",
    "        end = row[\"End Time [s]\"]\n",
    "        # Generate expected timestamps from start to end (inclusive) using step of 0.04.\n",
    "        # Adding half-step to ensure inclusion of the endpoint in floating-point arithmetic.\n",
    "        expected_times = np.arange(start, end + step/2, step)\n",
    "        # Round expected times for safe comparison.\n",
    "        expected_times = np.round(expected_times, 2)\n",
    "        expected_count = len(expected_times)\n",
    "        # Count how many expected timestamps are found in available_times.\n",
    "        found_count = sum(1 for t in expected_times if t in available_times)\n",
    "        missing_fraction = 1 - (found_count / expected_count) if expected_count > 0 else 0\n",
    "        \n",
    "        # If the fraction of missing timestamps is at most the threshold, keep the event.\n",
    "        if missing_fraction <= missing_threshold:\n",
    "            filtered_rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(filtered_rows)\n",
    "\n",
    "\n",
    "# Apply the filter function:\n",
    "filtered_pass_df = filter_pass_events_by_timestamp_coverage(event_data_passes, ball, step=0.04, missing_threshold=0.0)\n",
    "\n",
    "print(\"Filtered Pass Events:\")\n",
    "print(filtered_pass_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for outliers (e.g., z-score > 3 or < -3)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "# Create a density plot on the column 'my_column'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=ball, x='speed', fill=True, color='skyblue')\n",
    "plt.title('Density Plot of my_column')\n",
    "plt.xlabel('my_column')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_filter_speed(speed_data, process_variance=1e-3, measurement_variance=0.1):\n",
    "    \"\"\"\n",
    "    Apply a simple one-dimensional Kalman filter to a 1D array of speed measurements.\n",
    "    \n",
    "    Parameters:\n",
    "      speed_data (array-like): The measured speeds.\n",
    "      process_variance (float): Variance of the process noise.\n",
    "      measurement_variance (float): Variance of the measurement noise.\n",
    "      \n",
    "    Returns:\n",
    "      np.array: The filtered speed estimates.\n",
    "    \"\"\"\n",
    "    n = len(speed_data)\n",
    "    # Allocate arrays for estimates and error covariance.\n",
    "    xhat = np.zeros(n)  # Filtered estimate.\n",
    "    P = np.zeros(n)     # Estimate error covariance.\n",
    "    \n",
    "    # Initialize with the first measurement.\n",
    "    xhat[0] = speed_data[0]\n",
    "    P[0] = 1.0  # initial uncertainty\n",
    "    \n",
    "    for k in range(1, n):\n",
    "        # Prediction step: assume the state (speed) doesn't change\n",
    "        xhatminus = xhat[k-1]\n",
    "        Pminus = P[k-1] + process_variance\n",
    "        \n",
    "        # Update step: incorporate the measurement at time k\n",
    "        K = Pminus / (Pminus + measurement_variance)\n",
    "        xhat[k] = xhatminus + K * (speed_data[k] - xhatminus)\n",
    "        P[k] = (1 - K) * Pminus\n",
    "        \n",
    "    return xhat\n",
    "\n",
    "# Example usage:\n",
    "# Let's assume you have a DataFrame with a column 'speed'\n",
    "# Here we create some synthetic data for demonstration.\n",
    "\n",
    "# Apply the Kalman filter to the 'speed' column.\n",
    "ball['filtered_acceleration'] = kalman_filter_speed(ball['acceleration'].values, process_variance=1e-3, measurement_variance=0.01)\n",
    "\n",
    "\n",
    "# Plot the results.\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(ball['time'][0:200], ball['acceleration'][0:200], label='Original Speed', marker='o', linestyle='--', alpha=0.7)\n",
    "#plt.plot(ball['time'][0:200], ball['filtered_acceleration'][0:200], label='Filtered Speed', linewidth=2)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"acceleration\")\n",
    "plt.title(\"Kalman Filter Applied to Ball Acceleration\")\n",
    "plt.axhline(y=20, color='red', linestyle='--', linewidth=1.5)\n",
    "plt.axhline(y=-20, color='red', linestyle='--', linewidth=1.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aesthetic and clean plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(\n",
    "    ball['time'][0:200], \n",
    "    ball['acceleration'][0:200], \n",
    "    label='Original Acceleration', \n",
    "    marker='o', \n",
    "    linestyle='--', \n",
    "    alpha=0.6, \n",
    "    linewidth=2, \n",
    "    markersize=4\n",
    ")\n",
    "\n",
    "# Threshold lines\n",
    "plt.axhline(y=20, color='crimson', linestyle='--', linewidth=2, label='±20m/s² Threshold for kicks and recieval of the ball')\n",
    "plt.axhline(y=-20, color='crimson', linestyle='--', linewidth=2)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Time (s)\", fontsize=14)\n",
    "plt.ylabel(\"Acceleration (m/s²)\", fontsize=14)\n",
    "plt.title(\"Acceleration of the ball first 10 seconds\", fontsize=16, fontweight='bold')\n",
    "\n",
    "# Grid and legend\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "color1= \"blue\"\n",
    "color2 = \"darkviolet\"\n",
    "# Create figure and primary axis\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# --- Plot acceleration on the primary y-axis (left) ---\n",
    "ax1.plot(\n",
    "    ball['time'], \n",
    "    ball['acceleration'], \n",
    "    label='Original Acceleration', \n",
    "    marker='o', \n",
    "    linestyle='--', \n",
    "    alpha=0.6, \n",
    "    linewidth=2, \n",
    "    markersize=4,\n",
    "    color='dodgerblue'\n",
    ")\n",
    "# Labels and grid\n",
    "ax1.set_xlabel(\"Time (s)\", fontsize=14)\n",
    "ax1.set_ylabel(\"Acceleration (m/s²)\", fontsize=14, color='black')\n",
    "ax1.tick_params(axis='y', labelcolor='black')\n",
    "ax1.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "# --- Scatter markers and annotations ---\n",
    "\n",
    "# Interpolate to get y-values for scatter points\n",
    "start_times = event_data_passes['Start Time [s]'][2:4]\n",
    "end_times = event_data_passes['End Time [s]'][2:4]\n",
    "\n",
    "start_y = np.interp(start_times, ball['time'], ball['acceleration'])\n",
    "end_y = np.interp(end_times, ball['time'], ball['acceleration'])\n",
    "\n",
    "# Scatter markers\n",
    "ax1.scatter(start_times, start_y, color=color1, label='Start Time', s=60, zorder=5)\n",
    "ax1.scatter(end_times, end_y, color=color2, label='End Time', s=60, zorder=5)\n",
    "\n",
    "# Add text labels at 45 degrees\n",
    "for x, y in zip(start_times, start_y):\n",
    "    ax1.text(x-0.11, y + -60.5, 'kicked', rotation=45, color=color1, fontsize=10)\n",
    "\n",
    "for x, y in zip(end_times, end_y):\n",
    "    ax1.text(x, y + 20.5, 'recieved', rotation=45, color=color2, fontsize=10)\n",
    "\n",
    "# Title and legend\n",
    "plt.title(\"Ball Acceleration with markers for a kick (start of pass) and receival (end of pass)\", fontsize=16, fontweight='bold')\n",
    "\n",
    "# Combine and show legend\n",
    "#lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "#ax1.legend(lines_1, labels_1, loc='upper right', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "\n",
    "# Create a new column for the moving average smoothed acceleration\n",
    "ball['smoothed_acceleration'] = ball['acceleration'].rolling(window=window_size, center=True).mean()\n",
    "\n",
    "# Plot for comparison\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(ball['time'][0:200], ball['acceleration'][0:200], label='Original Acceleration', alpha=0.6)\n",
    "plt.plot(ball['time'][0:200], ball['smoothed_acceleration'][0:200], label='Smoothed (Moving Average)', linewidth=2)\n",
    "plt.axhline(y=5, color='red', linestyle='--', linewidth=1.5)\n",
    "plt.axhline(y=-5, color='red', linestyle='--', linewidth=1.5)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Acceleration\")\n",
    "plt.legend()\n",
    "plt.title(\"Acceleration Smoothing with Moving Average\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set moving average window\n",
    "window_size = 5\n",
    "ball['smoothed_acceleration'] = ball['acceleration'].rolling(window=window_size, center=True).mean()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Original acceleration\n",
    "plt.plot(\n",
    "    ball['time'][190:300], \n",
    "    ball['acceleration'][190:300], \n",
    "    label='Original Acceleration', \n",
    "    linestyle='--', \n",
    "    alpha=0.5, \n",
    "    linewidth=2, \n",
    "    color='gray'\n",
    ")\n",
    "\n",
    "# Smoothed acceleration\n",
    "plt.plot(\n",
    "    ball['time'][190:300], \n",
    "    ball['smoothed_acceleration'][190:300], \n",
    "    label=f'Smoothed (Moving Avg, window={window_size})', \n",
    "    linewidth=2.5, \n",
    "    color='dodgerblue'\n",
    ")\n",
    "\n",
    "# Threshold lines\n",
    "#plt.axhline(y=5, color='crimson', linestyle='--', linewidth=1.5, label='±5 Threshold')\n",
    "#plt.axhline(y=-5, color='crimson', linestyle='--', linewidth=1.5)\n",
    "\n",
    "# Labels and aesthetics\n",
    "plt.xlabel(\"Time (s)\", fontsize=14)\n",
    "plt.ylabel(\"Acceleration (m/s²)\", fontsize=14)\n",
    "plt.title(\"Acceleration Smoothing with Moving Average\", fontsize=16, fontweight='bold')\n",
    "\n",
    "# Grid, legend, layout\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.legend(fontsize=12, loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ball['smoothed_acceleration_observed'] = [1 if abs(x) >= 5 else 0 for x in ball['smoothed_acceleration']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.read_csv(\"/Users/annadaugaard/Desktop/VFF/explore/labelled_match_players_match.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_and_ball = players.merge(ball, on=\"time\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_and_ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_and_ball[\"smoothed_acceleration\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_and_ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_and_ball = players_and_ball.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#players_and_ball = players_and_ball.dropna()\n",
    "# Compute the Euclidean distance from the player (x, y) to the ball (ball_x, ball_y)\n",
    "players_and_ball[\"distance_to_ball\"] = np.sqrt((players_and_ball[\"x\"] - players_and_ball[\"ball_x\"])**2 +\n",
    "                                          (players_and_ball[\"y\"] - players_and_ball[\"ball_y\"])**2)\n",
    "\n",
    "# For each time point, rank the players by distance (1 = closest)\n",
    "players_and_ball[\"distance_rank\"] = players_and_ball.groupby(\"time\")[\"distance_to_ball\"].rank(method=\"min\")\n",
    "threshold = 3.0\n",
    "\n",
    "# For each time point, count how many players are within the threshold distance to the ball.\n",
    "# We use groupby with transform so that every row for the same time gets the same count.\n",
    "players_and_ball[\"uncertainty_index\"] = players_and_ball.groupby(\"time\")[\"distance_to_ball\"].transform(\n",
    "    lambda x: (x <= threshold).sum()\n",
    ")\n",
    "\n",
    "rank_1= players_and_ball[players_and_ball[\"distance_rank\"] == 1]\n",
    "rank_1_index= rank_1[rank_1[\"smoothed_acceleration_observed\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_1_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: group by time and id, then count occurrences\n",
    "grouped = rank_1_index[1:100].groupby([\"time\", \"id\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Create a stacked bar plot of counts.\n",
    "ax = grouped.plot(kind=\"bar\", stacked=True, figsize=(15, 6))\n",
    "plt.legend(title=\"Player ID\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "# Compute an uncertainty index per time.\n",
    "# For example, take the maximum uncertainty_index value at each time.\n",
    "uncertainty_by_time = rank_1_index[1:100].groupby(\"time\")[\"uncertainty_index\"].max()\n",
    "\n",
    "# Get the unique times (the index of 'grouped'); these serve as our x-axis ticks.\n",
    "unique_times = grouped.index\n",
    "x_positions = np.arange(len(unique_times))\n",
    "\n",
    "# Ensure the uncertainty series is aligned to the same unique_times order.\n",
    "uncertainty_by_time = uncertainty_by_time.loc[unique_times]\n",
    "\n",
    "bar_width = 0.8\n",
    "ax.hlines(y=uncertainty_by_time, \n",
    "          xmin=x_positions - bar_width/2, \n",
    "          xmax=x_positions + bar_width/2, \n",
    "          colors=\"black\", \n",
    "          linewidth=2, \n",
    "          label=\"Uncertainty Index\", \n",
    "          zorder=3)\n",
    "\n",
    "plt.xlabel(\"Time [s]\")\n",
    "plt.xticks(x_positions, [f\"{t:.2f}\" for t in unique_times], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there is more than 7 seconds between the first timestamp of a player having a ball, and the timestmap of another player having a ball do not count it as a pass\n",
    "\n",
    "### If there is multiple players close to the closest player, increase uncertainty score \n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def resolve_ties_by_team(df):\n",
    "    \"\"\"Resolve ties at the same timestamp by checking previous and next team's alignment.\"\"\"\n",
    "    unique_times = df[\"time\"].unique()\n",
    "    resolved = []\n",
    "    for i, t in enumerate(unique_times):\n",
    "        candidates = df[df[\"time\"] == t]\n",
    "        if len(candidates) == 1:\n",
    "            resolved.append(candidates.iloc[0])\n",
    "        else:\n",
    "            # If we have a previous candidate, use its team.\n",
    "            if resolved:\n",
    "                prev_team = resolved[-1][\"Team\"]\n",
    "            else:\n",
    "                prev_team = None\n",
    "\n",
    "            # Look at next unique time (if exists)\n",
    "            if i < len(unique_times) - 1:\n",
    "                next_time = unique_times[i+1]\n",
    "                next_candidates = df[df[\"time\"] == next_time]\n",
    "                next_team = next_candidates.iloc[0][\"Team\"] if len(next_candidates) > 0 else None\n",
    "            else:\n",
    "                next_team = None\n",
    "\n",
    "            chosen = None\n",
    "            # 1) Try matching both prev_team & next_team.\n",
    "            if prev_team and next_team:\n",
    "                both = candidates[(candidates[\"Team\"] == prev_team) & (candidates[\"Team\"] == next_team)]\n",
    "                if len(both) == 1:\n",
    "                    chosen = both.iloc[0]\n",
    "            # 2) If not, try matching prev_team.\n",
    "            if chosen is None and prev_team:\n",
    "                match_prev = candidates[candidates[\"Team\"] == prev_team]\n",
    "                if len(match_prev) == 1:\n",
    "                    chosen = match_prev.iloc[0]\n",
    "            # 3) If still not, try matching next_team.\n",
    "            if chosen is None and next_team:\n",
    "                match_next = candidates[candidates[\"Team\"] == next_team]\n",
    "                if len(match_next) == 1:\n",
    "                    chosen = match_next.iloc[0]\n",
    "            # 4) Fallback: choose the first candidate.\n",
    "            if chosen is None:\n",
    "                chosen = candidates.iloc[0]\n",
    "            resolved.append(chosen)\n",
    "    return pd.DataFrame(resolved).reset_index(drop=True)\n",
    "\n",
    "def compress_consecutive_id(df):\n",
    "    \"\"\"\n",
    "    Group consecutive rows with the same id into a single block with start/end times.\n",
    "    Only blocks with at least 3 observations (count >= 3) are retained.\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    current_block = None\n",
    "    for _, row in df.iterrows():\n",
    "        if current_block is None:\n",
    "            # Start a new block with count 1.\n",
    "            current_block = {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"Team\": row[\"Team\"],\n",
    "                \"start_time\": row[\"time\"],\n",
    "                \"end_time\": row[\"time\"],\n",
    "                \"count\": 1\n",
    "            }\n",
    "        else:\n",
    "            if row[\"id\"] == current_block[\"id\"]:\n",
    "                current_block[\"end_time\"] = row[\"time\"]\n",
    "                current_block[\"count\"] += 1\n",
    "            else:\n",
    "                # Only add the block if it has at least 3 observations.\n",
    "                if current_block[\"count\"] >= 3:\n",
    "                    blocks.append(current_block)\n",
    "                # Start a new block for the new id.\n",
    "                current_block = {\n",
    "                    \"id\": row[\"id\"],\n",
    "                    \"Team\": row[\"Team\"],\n",
    "                    \"start_time\": row[\"time\"],\n",
    "                    \"end_time\": row[\"time\"],\n",
    "                    \"count\": 1\n",
    "                }\n",
    "    if current_block and current_block[\"count\"] >= 3:\n",
    "        blocks.append(current_block)\n",
    "    return pd.DataFrame(blocks)\n",
    "\n",
    "def build_pass_events(blocks_df, rank_df, uncertainty_col=\"uncertainty_index\"):\n",
    "    \"\"\"\n",
    "    Create a pass event for each adjacent pair of blocks, but only if both blocks belong to the same team.\n",
    "    \n",
    "    For each pass event, defined as the transition between adjacent blocks in blocks_df,\n",
    "    we compute the mean uncertainty over the time interval from the start time of the current block\n",
    "    to the start time of the next block using values from rank_df.\n",
    "    \n",
    "    Returns a DataFrame with columns:\n",
    "      - \"Start Time [s]\"\n",
    "      - \"End Time [s]\"\n",
    "      - \"From\"\n",
    "      - \"To\"\n",
    "      - \"uncertainty\"\n",
    "      - \"Team\" (the team for the event)\n",
    "    \"\"\"\n",
    "    blocks_df = blocks_df.sort_values(\"start_time\").reset_index(drop=True)\n",
    "    events = []\n",
    "    for i in range(len(blocks_df) - 1):\n",
    "        # Only create a pass event if both blocks are on the same team.\n",
    "        if blocks_df.loc[i, \"Team\"] != blocks_df.loc[i+1, \"Team\"]:\n",
    "            continue\n",
    "        start_time = blocks_df.loc[i, \"start_time\"]\n",
    "        end_time = blocks_df.loc[i+1, \"start_time\"]\n",
    "        # Filter rows from rank_df with times between start_time and end_time.\n",
    "        subset = rank_df[(rank_df[\"time\"] >= start_time) & (rank_df[\"time\"] <= end_time)]\n",
    "        uncertainty_value = subset[uncertainty_col].mean() if not subset.empty else np.nan\n",
    "        events.append({\n",
    "            \"Start Time [s]\": start_time,\n",
    "            \"End Time [s]\": end_time,\n",
    "            \"From\": blocks_df.loc[i, \"id\"],\n",
    "            \"To\": blocks_df.loc[i+1, \"id\"],\n",
    "            \"uncertainty\": uncertainty_value,\n",
    "            \"Team\": blocks_df.loc[i, \"Team\"]\n",
    "        })\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "# -------------------------------\n",
    "# Example usage:\n",
    "# Assume 'rank_1_index' is your original DataFrame with at least columns \"time\", \"id\", \"Team\", and \"uncertainty_index\".\n",
    "\n",
    "# 1) Resolve ties in your DataFrame.\n",
    "df_resolved = resolve_ties_by_team(rank_1_index)\n",
    "\n",
    "# 2) Compress consecutive rows by id but only keep blocks with at least 3 observations.\n",
    "df_blocks = compress_consecutive_id(df_resolved)\n",
    "\n",
    "# 3) Build pass events from the valid blocks, computing uncertainty over the interval.\n",
    "#    Only events where both blocks belong to the same team are kept.\n",
    "df_passes = build_pass_events(df_blocks, rank_1_index, uncertainty_col=\"uncertainty_index\")\n",
    "\n",
    "df_passes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where (End Time [s] - Start Time [s]) <= 7\n",
    "df_filtered = df_passes[(df_passes[\"End Time [s]\"] - df_passes[\"Start Time [s]\"]) <= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data_passes_subset = filtered_pass_df[[\"Start Time [s]\", \"End Time [s]\",\"From\", \"To\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(event_data_passes_subset[\"End Time [s]\"] - event_data_passes_subset[\"Start Time [s]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WEIGHTED TRIANGLE MODEL\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create a triangle in 2D space\n",
    "# Points: ball (0,0), left base (-1, 3), right base (1, 3)\n",
    "ball = np.array([0, 0])\n",
    "left = np.array([-1.5, 3])\n",
    "right = np.array([1.5, 3])\n",
    "\n",
    "# Define a grid over the triangle bounding box\n",
    "x = np.linspace(-1.5, 1.5, 100)\n",
    "y = np.linspace(0, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Barycentric coordinates to check if a point is inside triangle\n",
    "def point_in_triangle(px, py, a, b, c):\n",
    "    v0 = c - a\n",
    "    v1 = b - a\n",
    "    v2 = np.array([px, py]) - a\n",
    "\n",
    "    dot00 = np.dot(v0, v0)\n",
    "    dot01 = np.dot(v0, v1)\n",
    "    dot02 = np.dot(v0, v2)\n",
    "    dot11 = np.dot(v1, v1)\n",
    "    dot12 = np.dot(v1, v2)\n",
    "\n",
    "    denom = dot00 * dot11 - dot01 * dot01\n",
    "    if denom == 0:\n",
    "        return False\n",
    "\n",
    "    u = (dot11 * dot02 - dot01 * dot12) / denom\n",
    "    v = (dot00 * dot12 - dot01 * dot02) / denom\n",
    "    return (u >= 0) and (v >= 0) and (u + v <= 1)\n",
    "\n",
    "# Create the \"height\" function: high at y=0 and y=3, low in the middle\n",
    "def triangle_density(x, y):\n",
    "    norm_y = y / 3  # normalize y to 0-1\n",
    "    return np.exp(-((norm_y - 0.15) ** 2) * 10) + np.exp(-((norm_y - 0.85) ** 2) * 10)\n",
    "\n",
    "# Apply mask and height function\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        if point_in_triangle(X[i, j], Y[i, j], ball, left, right):\n",
    "            Z[i, j] = triangle_density(X[i, j], Y[i, j])\n",
    "\n",
    "# Plot the triangle with the density profile\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none', alpha=0.9)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Weight (Density)')\n",
    "ax.set_title('3D Weighted Triangle – High Density at Tip and Base')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "# --- Triangle Definition ---\n",
    "ball = np.array([0, 0])\n",
    "left = np.array([-1.5, 3])\n",
    "right = np.array([1.5, 3])\n",
    "\n",
    "# Grid for evaluation\n",
    "x = np.linspace(-1.6, 1.6, 300)\n",
    "y = np.linspace(-0.1, 3.1, 300)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Barycentric check\n",
    "def point_in_triangle(px, py, a, b, c):\n",
    "    v0, v1 = c - a, b - a\n",
    "    v2 = np.array([px, py]) - a\n",
    "    dot00, dot01 = np.dot(v0, v0), np.dot(v0, v1)\n",
    "    dot02, dot11 = np.dot(v0, v2), np.dot(v1, v1)\n",
    "    dot12 = np.dot(v1, v2)\n",
    "    denom = dot00 * dot11 - dot01 * dot01\n",
    "    if denom == 0:\n",
    "        return False\n",
    "    u = (dot11 * dot02 - dot01 * dot12) / denom\n",
    "    v = (dot00 * dot12 - dot01 * dot02) / denom\n",
    "    return (u >= 0) and (v >= 0) and (u + v <= 1)\n",
    "\n",
    "# Triangle weight function\n",
    "def triangle_density(x, y):\n",
    "    norm_y = y / 3\n",
    "    return np.exp(-((norm_y - 0.15) ** 2) * 10) + np.exp(-((norm_y - 0.85) ** 2) * 10)\n",
    "\n",
    "# Mask and apply density\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        if point_in_triangle(X[i, j], Y[i, j], ball, left, right):\n",
    "            Z[i, j] = triangle_density(X[i, j], Y[i, j])\n",
    "\n",
    "# --- Plotting ---\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Surface\n",
    "surf = ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none', alpha=0.95, antialiased=True)\n",
    "\n",
    "# Highlight triangle edges\n",
    "for start, end in [(ball, left), (left, right), (right, ball)]:\n",
    "    ax.plot([start[0], end[0]], [start[1], end[1]], [0, 0], color='black', linewidth=2)\n",
    "\n",
    "# Labels and view\n",
    "ax.set_xlabel('X', labelpad=10, fontsize=12)\n",
    "ax.set_ylabel('Y', labelpad=10, fontsize=12)\n",
    "ax.set_zlabel('Weight (Density)', labelpad=10, fontsize=12)\n",
    "ax.set_title('Weighted Triangle Model\\nHigh Density at Tip and Base', fontsize=14, pad=20)\n",
    "ax.view_init(elev=35, azim=-60)\n",
    "ax.grid(False)\n",
    "\n",
    "# Remove axes spines for cleaner look\n",
    "ax.xaxis.pane.fill = False\n",
    "ax.yaxis.pane.fill = False\n",
    "ax.zaxis.pane.fill = False\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Triangle points\n",
    "ball = np.array([0, 0])\n",
    "left = np.array([-1.5, 3])\n",
    "right = np.array([1.5, 3])\n",
    "\n",
    "# Grid setup\n",
    "x = np.linspace(-1.6, 1.6, 300)\n",
    "y = np.linspace(-0.1, 3.1, 300)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "def point_in_triangle(px, py, a, b, c):\n",
    "    v0, v1 = c - a, b - a\n",
    "    v2 = np.array([px, py]) - a\n",
    "    dot00, dot01 = np.dot(v0, v0), np.dot(v0, v1)\n",
    "    dot02, dot11 = np.dot(v0, v2), np.dot(v1, v1)\n",
    "    dot12 = np.dot(v1, v2)\n",
    "    denom = dot00 * dot11 - dot01 * dot01\n",
    "    if denom == 0:\n",
    "        return False\n",
    "    u = (dot11 * dot02 - dot01 * dot12) / denom\n",
    "    v = (dot00 * dot12 - dot01 * dot02) / denom\n",
    "    return (u >= 0) and (v >= 0) and (u + v <= 1)\n",
    "\n",
    "def triangle_density(x, y):\n",
    "    norm_y = y / 3\n",
    "    return np.exp(-((norm_y - 0.15) ** 2) * 10) + np.exp(-((norm_y - 0.85) ** 2) * 10)\n",
    "\n",
    "# Apply mask to triangle only\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        if point_in_triangle(X[i, j], Y[i, j], ball, left, right):\n",
    "            Z[i, j] = triangle_density(X[i, j], Y[i, j])\n",
    "        else:\n",
    "            Z[i, j] = np.nan  # Hide outside triangle\n",
    "\n",
    "# --- Plot only the surface ---\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Surface only, no edge lines or grid\n",
    "surf = ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none', alpha=1, antialiased=True)\n",
    "\n",
    "# Clean up axes\n",
    "ax.set_xlabel('X', labelpad=10, fontsize=12)\n",
    "ax.set_ylabel('Y', labelpad=10, fontsize=12)\n",
    "ax.set_zlabel('Weight', labelpad=10, fontsize=12)\n",
    "ax.set_title('Weighted Triangle Surface – Tip & Base Emphasis', fontsize=14, pad=20)\n",
    "ax.view_init(elev=40, azim=-60)\n",
    "ax.set_box_aspect([2, 2, 1])  # Equal aspect ratio\n",
    "\n",
    "# Optional: hide the panes to make it float\n",
    "ax.xaxis.pane.fill = False\n",
    "ax.yaxis.pane.fill = False\n",
    "ax.zaxis.pane.fill = False\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_pass_event_matches(df1, df2, tolerance=15):\n",
    "    \"\"\"\n",
    "    Compare two pass event DataFrames (df1 and df2) and count how many events in df1\n",
    "    have a matching event in df2. An event in df1 is considered a match if there exists\n",
    "    an event in df2 such that:\n",
    "      - |df1[\"Start Time [s]\"] - df2[\"Start Time [s]\"]| <= tolerance\n",
    "      - |df1[\"End Time [s]\"] - df2[\"End Time [s]\"]| <= tolerance\n",
    "      - df1[\"From\"] == df2[\"From\"] and df1[\"To\"] == df2[\"To\"]\n",
    "      \n",
    "    Parameters:\n",
    "      df1 (pd.DataFrame): Ground-truth pass events.\n",
    "      df2 (pd.DataFrame): Predicted pass events.\n",
    "      tolerance (float): Tolerance (in seconds) allowed on the start and end times.\n",
    "      \n",
    "    Returns:\n",
    "      dict: A dictionary with keys:\n",
    "            - \"match_count\": number of events in df1 with a matching event in df2\n",
    "            - \"total_df1\": total events in df1\n",
    "            - \"total_df2\": total events in df2\n",
    "            - \"recall\": (matches / total_df1) * 100 (% of ground-truth events found in predictions)\n",
    "            - \"precision\": (matches / total_df2) * 100 (% of predicted events that are correct)\n",
    "            - \"f1_score\": the F1 score based on recall and precision.\n",
    "    \"\"\"\n",
    "    matches = 0\n",
    "    # For each event in df1, look for a matching event in df2\n",
    "    for i, row1 in df1.iterrows():\n",
    "        for j, row2 in df2.iterrows():\n",
    "            if (abs(row1[\"Start Time [s]\"] - row2[\"Start Time [s]\"]) <= tolerance and\n",
    "                abs(row1[\"End Time [s]\"] - row2[\"End Time [s]\"]) <= tolerance and\n",
    "                row1[\"From\"] == row2[\"From\"] and\n",
    "                row1[\"To\"] == row2[\"To\"]):\n",
    "                matches += 1\n",
    "                break  # Once a match is found for this event, move to the next event in df1.\n",
    "\n",
    "    total_df1 = len(df1)\n",
    "    total_df2 = len(df2)\n",
    "    recall = (matches / total_df1) * 100 if total_df1 > 0 else 0\n",
    "    precision = (matches / total_df2) * 100 if total_df2 > 0 else 0\n",
    "    f1_score = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    summary = {\n",
    "        \"match_count\": matches,\n",
    "        \"total_df1\": total_df1,\n",
    "        \"total_df2\": total_df2,\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1_score\": f1_score\n",
    "    }\n",
    "    \n",
    "    print(\"Evaluation Metrics:\")\n",
    "    print(f\"Total events in labelled data (ground truth): {total_df1}\")\n",
    "    print(f\"Total events in predicted (predictions): {total_df2}\")\n",
    "    print(f\"Match Count: {matches}\")\n",
    "    print(f\"Recall: {recall:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}%\")\n",
    "    print(f\"F1 Score: {f1_score:.2f}%\")\n",
    "    \n",
    "    return summary\n",
    "# Evaluate matches:\n",
    "metrics = count_pass_event_matches(event_data_passes_subset, df_filtered, tolerance=6)\n",
    "\n",
    "# Precision measures how many of the items your model identified as positive (or relevant) are actually correct.\n",
    "# Recall measures how many of the actual positive (or relevant) items your model was able to capture.\n",
    "\n",
    "\n",
    "### CHOICES: MISSING TIMESTAMPS NÅR JEG VURDERE, THREHOLD = 0? \n",
    "### TOLERENCE PÅ 8 SEKUNDER?\n",
    "### MAKSIMUM DURATION AF PASS? 10SEKUNDER? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_pass_event_matches(df1, df2, tolerance=15):\n",
    "    \"\"\"\n",
    "    Compare two pass event DataFrames (df1 and df2) and count how many events in df1\n",
    "    have a matching event in df2. An event in df1 is considered a match if there exists\n",
    "    an event in df2 such that:\n",
    "      - |df1[\"Start Time [s]\"] - df2[\"Start Time [s]\"]| <= tolerance\n",
    "      - |df1[\"End Time [s]\"] - df2[\"End Time [s]\"]| <= tolerance\n",
    "      - df1[\"From\"] == df2[\"From\"] and df1[\"To\"] == df2[\"To\"]\n",
    "      \n",
    "    Parameters:\n",
    "      df1 (pd.DataFrame): Ground-truth pass events.\n",
    "      df2 (pd.DataFrame): Predicted pass events.\n",
    "      tolerance (float): Tolerance (in seconds) allowed on the start and end times.\n",
    "      \n",
    "    Returns:\n",
    "      dict: A dictionary with keys:\n",
    "            - \"match_count\": number of events in df1 with a matching event in df2\n",
    "            - \"total_df1\": total events in df1\n",
    "            - \"total_df2\": total events in df2\n",
    "            - \"recall\": (matches / total_df1) * 100 (% of ground-truth events found in predictions)\n",
    "            - \"precision\": (matches / total_df2) * 100 (% of predicted events that are correct)\n",
    "            - \"f1_score\": the F1 score based on recall and precision.\n",
    "    \"\"\"\n",
    "    matches = 0\n",
    "    # For each event in df1, look for a matching event in df2\n",
    "    for i, row1 in df1.iterrows():\n",
    "        for j, row2 in df2.iterrows():\n",
    "            if (abs(row1[\"Start Time [s]\"] - row2[\"Start Time [s]\"]) <= tolerance and\n",
    "                abs(row1[\"End Time [s]\"] - row2[\"End Time [s]\"]) <= tolerance and\n",
    "                row1[\"From\"] == row2[\"From\"] and\n",
    "                row1[\"To\"] == row2[\"To\"]):\n",
    "                matches += 1\n",
    "                break  # Once a match is found for this event, move to the next event in df1.\n",
    "\n",
    "    total_df1 = len(df1)\n",
    "    total_df2 = len(df2)\n",
    "    recall = (matches / total_df1) * 100 if total_df1 > 0 else 0\n",
    "    precision = (matches / total_df2) * 100 if total_df2 > 0 else 0\n",
    "    f1_score = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    summary = {\n",
    "        \"match_count\": matches,\n",
    "        \"total_df1\": total_df1,\n",
    "        \"total_df2\": total_df2,\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1_score\": f1_score\n",
    "    }\n",
    "    \n",
    "    print(\"Evaluation Metrics:\")\n",
    "    print(f\"Total events in labelled data (ground truth): {total_df1}\")\n",
    "    print(f\"Total events in predicted (predictions): {total_df2}\")\n",
    "    print(f\"Match Count: {matches}\")\n",
    "    print(f\"Recall: {recall:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}%\")\n",
    "    print(f\"F1 Score: {f1_score:.2f}%\")\n",
    "    \n",
    "    return summary\n",
    "# Evaluate matches:\n",
    "metrics = count_pass_event_matches(event_data_passes_subset, df_filtered, tolerance=6)\n",
    "\n",
    "# Precision measures how many of the items your model identified as positive (or relevant) are actually correct.\n",
    "# Recall measures how many of the actual positive (or relevant) items your model was able to capture.\n",
    "\n",
    "\n",
    "### CHOICES: MISSING TIMESTAMPS NÅR JEG VURDERE, THREHOLD = 0? \n",
    "### TOLERENCE PÅ 8 SEKUNDER?\n",
    "### MAKSIMUM DURATION AF PASS? 10SEKUNDER? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_matches(pred_event, gt_event, tolerance):\n",
    "    \"\"\"\n",
    "    Returns True if the predicted event matches the ground truth event\n",
    "    within the given tolerance for start and end times, and if the From and To values match exactly.\n",
    "    \"\"\"\n",
    "    return (abs(pred_event[\"Start Time [s]\"] - gt_event[\"Start Time [s]\"]) <= tolerance and\n",
    "            abs(pred_event[\"End Time [s]\"] - gt_event[\"End Time [s]\"]) <= tolerance and\n",
    "            pred_event[\"From\"] == gt_event[\"From\"] and\n",
    "            pred_event[\"To\"] == gt_event[\"To\"])\n",
    "\n",
    "def evaluate_predictions_with_uncertainty(pred_df, gt_df, tolerance):\n",
    "    \"\"\"\n",
    "    For each predicted event in pred_df, determine if it matches any ground truth event in gt_df.\n",
    "    \n",
    "    Returns:\n",
    "      - confusion: a dictionary with TP, FP, and FN counts.\n",
    "      - correct_uncertainties: list of uncertainty values for predicted events that are true positives.\n",
    "      - incorrect_uncertainties: list of uncertainty values for predicted events that are false positives.\n",
    "    \n",
    "    Here, a predicted event is considered correct (TP) if there is at least one ground truth event\n",
    "    that matches its Start and End times within the given tolerance and has the same From and To values.\n",
    "    Ground truth events not matched by any prediction are counted as FN.\n",
    "    \"\"\"\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    matched_gt = set()  # Indices of ground truth events that were matched\n",
    "    correct_uncertainties = []\n",
    "    incorrect_uncertainties = []\n",
    "    \n",
    "    # For each predicted event, try to find a matching ground truth event.\n",
    "    for i, pred in pred_df.iterrows():\n",
    "        match_found = False\n",
    "        for j, gt in gt_df.iterrows():\n",
    "            if event_matches(pred, gt, tolerance):\n",
    "                match_found = True\n",
    "                matched_gt.add(j)\n",
    "                break  # Stop at first match for this prediction.\n",
    "        if match_found:\n",
    "            TP += 1\n",
    "            correct_uncertainties.append(pred[\"uncertainty\"])\n",
    "        else:\n",
    "            FP += 1\n",
    "            incorrect_uncertainties.append(pred[\"uncertainty\"])\n",
    "    \n",
    "    FN = len(gt_df) - len(matched_gt)\n",
    "    \n",
    "    confusion = {\"TP\": TP, \"FP\": FP, \"FN\": FN}\n",
    "    return confusion, correct_uncertainties, incorrect_uncertainties\n",
    "\n",
    "# Example data frames (replace these with your actual data)\n",
    "# Set tolerance (in seconds) for matching event times.\n",
    "tolerance = 7\n",
    "\n",
    "# Evaluate predictions and get confusion metrics along with uncertainty values.\n",
    "confusion, correct_unc, incorrect_unc = evaluate_predictions_with_uncertainty( df_filtered,event_data_passes_subset, tolerance)\n",
    "\n",
    "print(\"Confusion Metrics:\")\n",
    "print(confusion)\n",
    "\n",
    "# Optionally, plot the distribution of uncertainty values for correct vs. incorrect predictions.\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.boxplot([correct_unc, incorrect_unc], labels=[\"Correct Predictions\", \"Incorrect Predictions\"])\n",
    "plt.ylabel(\"Uncertainty\")\n",
    "plt.title(\"Uncertainty Distribution: Correct vs. Incorrect Predictions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Create a DataFrame where 'error' is 0 for correct and 1 for error:\n",
    "df_correct = pd.DataFrame({\"uncertainty\": correct_unc, \"error\": 0})\n",
    "df_incorrect = pd.DataFrame({\"uncertainty\": incorrect_unc, \"error\": 1})\n",
    "reg_df = pd.concat([df_correct, df_incorrect], ignore_index=True)\n",
    "\n",
    "# Add an intercept column:\n",
    "reg_df[\"intercept\"] = 1.0\n",
    "logit_model = sm.Logit(reg_df[\"error\"], reg_df[[\"intercept\", \"uncertainty\"]])\n",
    "result = logit_model.fit(disp=False)  # disp=False to suppress fitting output\n",
    "\n",
    "print(result.summary())\n",
    "\n",
    "# Extract the p-value for uncertainty:\n",
    "p_value_uncertainty = result.pvalues[\"uncertainty\"]\n",
    "print(f\"\\nP-value for uncertainty coefficient: {p_value_uncertainty:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonVFF",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
